{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ec24aa1bb7febbb33e962ce4edfa8a7cff2b3d9"
   },
   "outputs": [],
   "source": [
    "#more anchor boxees & focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cdc8e33764ac50f47168b856e889568427821bfd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dce049e9471b40e86e5291b3f802cd7738b50c01"
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1797274b7094bbd514d1bd8e5aa4af8a16b4ce74"
   },
   "outputs": [],
   "source": [
    "PATH_ANNO = r'../input/pascal_voc/PASCAL_VOC/pascal_train2007.json'\n",
    "PATH_IMG = r'../input/voctrainval_06-nov-2007/VOCdevkit/VOC2007/JPEGImages'\n",
    "# PATH_ANNO = r'/home/spacor/fastai/courses/dl2/data/PASCAL_VOC/pascal_train2007.json'\n",
    "# PATH_IMG = r'/home/spacor/fastai/courses/dl2/data/PASCAL_VOC/VOCdevkit/VOC2007/JPEGImages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fed057b0293ffb37dd3dea080ded4225a0190cd"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "VAL_SIZE =0.33\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "404c3af19887ff0e1ca607b23055ca8d6e5d91e4"
   },
   "outputs": [],
   "source": [
    "def get_img_annotations(path_annotations):\n",
    "    \"\"\"\n",
    "    load raw img annotations\n",
    "    bbox format is x-min, y-min, w, h\n",
    "    \"\"\"\n",
    "    anno_raw=json.load(open(path_annotations))\n",
    "    imgId_anno_map = defaultdict(list)\n",
    "    for i in anno_raw['annotations']:\n",
    "        imgId_anno_map[i['image_id']].append(i)\n",
    "    img_anno = []\n",
    "    for i in anno_raw['images']:\n",
    "        tmp_anno = imgId_anno_map[i['id']]\n",
    "        tmp_rcd = i\n",
    "        tmp_rcd['annotation'] = tmp_anno\n",
    "        img_anno.append(tmp_rcd)\n",
    "    return img_anno\n",
    "\n",
    "def transform_anno(raw_annotations):\n",
    "    \"\"\"\n",
    "    transform raw img annotation into transformed annotation\n",
    "    transform bbox format from x-min, y-min, w, h to\n",
    "    y-min, x-min, y-max, x-max\n",
    "    \"\"\"\n",
    "    transformed_annotations = []\n",
    "    for raw_annotation in raw_annotations:\n",
    "        tfm_bbox = lambda raw_bbox: (raw_bbox[1], raw_bbox[0], raw_bbox[3]+raw_bbox[1]-1, raw_bbox[2]+raw_bbox[0]-1)\n",
    "        tmp_anno = {'filename': raw_annotation['file_name'],\n",
    "                   'bboxes': [{'bbox': tfm_bbox(i['bbox']), 'category_id': i['category_id']} for i in raw_annotation['annotation']]\n",
    "                   }\n",
    "        transformed_annotations.append(tmp_anno)\n",
    "    return transformed_annotations\n",
    "\n",
    "def get_obj_category_mappings(path_annotations):\n",
    "    anno_raw=json.load(open(path_annotations))\n",
    "    catId_name_map = dict()\n",
    "    for i in anno_raw['categories']:\n",
    "        catId_name_map[i['id']] = i['name']\n",
    "    return catId_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cbaa65b71ca6671ba4b29ecc5f4a5d66a0fb5d7"
   },
   "outputs": [],
   "source": [
    "#transform bbox convention functions\n",
    "\n",
    "def bbtfm_bb2wh(bbox):\n",
    "    \"\"\"\n",
    "    transform bbox from\n",
    "    y-min, x-min, y-max, x-max\n",
    "    to\n",
    "    x-min, y-min, w, h\n",
    "    \"\"\"\n",
    "    alt_bb_fmt = bbox[1], bbox[0], bbox[3] - bbox[1] - 1, bbox[2] - bbox[0] - 1\n",
    "    return alt_bb_fmt\n",
    "\n",
    "def bbtfm_bb2xy(bbox):\n",
    "    \"\"\"\n",
    "    transform bbox from\n",
    "    y-min, x-min, y-max, x-max\n",
    "    to\n",
    "    x-min, y-min, x-max, y-max\n",
    "    \"\"\"\n",
    "    alt_bb_fmt = bbox[1], bbox[0], bbox[3], bbox[2]\n",
    "    return alt_bb_fmt\n",
    "\n",
    "def bbtfm_xy2bb(bbox):\n",
    "    \"\"\"\n",
    "    transform bbox from\n",
    "    x-min, y-min, x-max, y-max\n",
    "    to\n",
    "    y-min, x-min, y-max, x-max\n",
    "    \"\"\"\n",
    "    alt_bb_fmt = bbox[1], bbox[0], bbox[3], bbox[2]\n",
    "    return alt_bb_fmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "544e2369919c3fec7ce689c6c3864f610e1edc5f"
   },
   "outputs": [],
   "source": [
    "img_anno = transform_anno(get_img_annotations(PATH_ANNO)) #img annotation\n",
    "img_cate_map = get_obj_category_mappings(PATH_ANNO) #cat ID to cat des\n",
    "img_cate_map[0] = 'BG'\n",
    "\n",
    "MAX_BBOX = int(np.array([len(i['bboxes']) for i in img_anno]).max()) #max num of bbox #max number of bounding box in sample\n",
    "NUM_CLASS = len(img_cate_map) #num class, reserve 0 for background\n",
    "NUM_CLASS_wo_BG = NUM_CLASS - 1\n",
    "img_anno[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "930cea2f6dbbd54b13a7ebd20d352d92d666f4ad"
   },
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    return ax\n",
    "\n",
    "def draw_rect(ax, b):\n",
    "    \"\"\"\n",
    "    b: bounding box (list) top left point coordinate, width, height (x, y, w, h)\n",
    "    \"\"\"\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor='white', lw=2))\n",
    "    draw_outline(patch, 4)\n",
    "\n",
    "def draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "def draw_text(ax, xy, txt, sz=14):\n",
    "    text = ax.text(*xy, txt,verticalalignment='top', color='white', fontsize=sz, weight='bold')\n",
    "    draw_outline(text, 1)\n",
    "\n",
    "def show_img_bbs(img, bboxes, texts):\n",
    "    \"\"\"\n",
    "    Show img alongside with all bboxes\n",
    "    img: np array of image\n",
    "    bboxes: list of bbox in y-min, x-min, y-max, x-max\n",
    "    texts: list of annotation (categories) of bbox\n",
    "    \"\"\"\n",
    "    ax = show_img(img)\n",
    "    for tmp_bbox, tmp_text in zip(bboxes, texts):\n",
    "        bb = bbtfm_bb2wh(tmp_bbox)\n",
    "        draw_rect(ax, bb)\n",
    "        draw_text(ax, bb[:2], tmp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7cf12e94b2c14cc16f4898a4c13f48ed1768551"
   },
   "outputs": [],
   "source": [
    "#test show image and bounding box\n",
    "idx = 5\n",
    "tmp = img_anno[idx]\n",
    "tmp_img = io.imread(os.path.join(PATH_IMG, tmp['filename']))\n",
    "tmp_bb = [i['bbox'] for i in tmp['bboxes']]\n",
    "tmp_cat = [img_cate_map[i['category_id']] for i in tmp['bboxes']]\n",
    "\n",
    "show_img_bbs(tmp_img, tmp_bb, tmp_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa24dd0561526b91e3d2baa2af527bb3d36551d1"
   },
   "outputs": [],
   "source": [
    "#Test augmentation - img only\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "    iaa.Sequential([\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.Sometimes(0.5,\n",
    "            iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "        ),\n",
    "        iaa.ContrastNormalization((0.75, 1.5)),\n",
    "        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "        iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "        iaa.Affine(\n",
    "            rotate=(-10, 10),\n",
    "        )\n",
    "    ], random_order=True) # apply augmenters in random order\n",
    "], random_order=False)\n",
    "\n",
    "tmp_tfm_img = seq.augment_images([tmp_img.copy(),tmp_img.copy()])\n",
    "show_img_bbs(tmp_tfm_img[0], [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c05aa20f6bdc28d0cbe310cf80d79493fb75afc5"
   },
   "outputs": [],
   "source": [
    "#Test augmentation - img + bbox\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Scale({\"height\": IMG_SIZE, \"width\": IMG_SIZE}),\n",
    "    iaa.Sequential([\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.Sometimes(0.5,\n",
    "            iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "        ),\n",
    "        iaa.ContrastNormalization((0.75, 1.5)),\n",
    "        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "        iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "        iaa.Affine(\n",
    "            rotate=(-10, 10),\n",
    "        )\n",
    "    ], random_order=True) # apply augmenters in random order\n",
    "], random_order=False)\n",
    "# convert_bb_fmt = lambda bb: [bb[0], bb[1], bb[0] + bb[2], bb[1] + bb[3]] #convert topleft xy, wh to min x, min y, max x,  max y\n",
    "\n",
    "#read raw data\n",
    "idx = 15\n",
    "tmp = img_anno[idx]\n",
    "tmp_img = io.imread(os.path.join(PATH_IMG, tmp['filename']))\n",
    "tmp_bb = [i['bbox'] for i in tmp['bboxes']]\n",
    "tmp_bb_obj = ia.BoundingBoxesOnImage([ia.BoundingBox(*bbtfm_bb2xy(i)) for i in tmp_bb], shape=tmp_img.shape) #convert to Boundingboxes object\n",
    "tmp_cat = [img_cate_map[i['category_id']] for i in tmp['bboxes']]\n",
    "\n",
    "#augmentation\n",
    "extract_bb = lambda bb_obj: [bb_obj.y1_int, bb_obj.x1_int, bb_obj.y2_int, bb_obj.x2_int]\n",
    "\n",
    "seq_det = seq.to_deterministic()\n",
    "\n",
    "tmp_aug_img = seq_det.augment_images([tmp_img.copy()])[0]\n",
    "tmp_aug_bbs_raw = seq_det.augment_bounding_boxes([tmp_bb_obj])[0]\n",
    "tmp_aug_bbs=[]\n",
    "for b in tmp_aug_bbs_raw.bounding_boxes:\n",
    "    if b.is_out_of_image(tmp_aug_img):\n",
    "        bb = [0.0,0.0,0.0,0.0] #dummy for out of image bb\n",
    "        print('bb out of bound')\n",
    "    else:\n",
    "        bb = extract_bb(b.cut_out_of_image(tmp_aug_img))\n",
    "    tmp_aug_bbs.append(bb)\n",
    "\n",
    "# #display\n",
    "show_img_bbs(tmp_aug_img, tmp_aug_bbs, tmp_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f03742b4d579b6108d81a5adc24187612b4d5d71"
   },
   "outputs": [],
   "source": [
    "class PascalDataset(Dataset):\n",
    "    def __init__(self, img_meta, transform = None):\n",
    "        self.img_meta = img_meta\n",
    "        self.transform = transform\n",
    "        self.num_bboxes = 5\n",
    "#         self.num_bboxes = MAX_BBOX\n",
    "        self.bbox_co_len = 4 #number of parameter defines a bbox\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_meta)\n",
    "    \n",
    "#     def _pad(self, original_seq, size, padding):\n",
    "#         seq = (original_seq + [padding] * abs((len(original_seq)-size)))[:size]\n",
    "#         return seq\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = io.imread(os.path.join(PATH_IMG, self.img_meta[idx]['filename']))\n",
    "        \n",
    "#         bboxes = [i['bbox'] for i in self.img_meta[idx]['bboxes']]\n",
    "#         bboxes = self._pad(bboxes, self.num_bboxes, [0,0,1,1]) #padding; dummy for pad\n",
    "#         categories = [i['category_id'] for i in self.img_meta[idx]['bboxes']]\n",
    "#         categories = self._pad(categories, self.num_bboxes, 0) #padding; dummy for pad\n",
    "        \n",
    "#         if bool(self.transform) is True:\n",
    "#             img_boxes = (img, bboxes)\n",
    "#             img, bboxes = self.transform(img_boxes)\n",
    "        \n",
    "#         categories = torch.tensor(categories)\n",
    "    \n",
    "        #read bboxes\n",
    "        bboxes = [i['bbox'] for i in self.img_meta[idx]['bboxes']]\n",
    "        \n",
    "        #read cate\n",
    "        categories = [i['category_id'] for i in self.img_meta[idx]['bboxes']]\n",
    "        \n",
    "        #augmentation\n",
    "        if bool(self.transform) is True:\n",
    "            img_bb_cat = (img, bboxes, categories)\n",
    "            img, bboxes, categories = self.transform(img_bb_cat)\n",
    "            \n",
    "        #pad\n",
    "        bboxes = np.pad(np.array(bboxes).flatten(), pad_width = (0, self.num_bboxes * self.bbox_co_len), mode = 'constant', constant_values=(0,0))[:self.num_bboxes * self.bbox_co_len]\n",
    "        categories = np.pad(np.array(categories), pad_width = (0, self.num_bboxes), mode = 'constant', constant_values=(0,0))[:self.num_bboxes]\n",
    "        \n",
    "        #transform to tensor\n",
    "        img = img #already transformed to torch tensor in transformation\n",
    "        bboxes = torch.from_numpy(bboxes).float()\n",
    "        categories = torch.from_numpy(categories)\n",
    "        \n",
    "        output = (img, (bboxes, categories))\n",
    "        return output\n",
    "\n",
    "class NormalizeImg:\n",
    "    def __init__(self):\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "    def __call__(self, img_bb_cat):\n",
    "        img, bboxes, categories = img_bb_cat\n",
    "        \n",
    "        \n",
    "        #transform img to tensor\n",
    "        img = np.clip(img, a_min=0, a_max=None)\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        img = torch.from_numpy(img).float().div(255)\n",
    "        #normalize\n",
    "        img = self.normalize(img)\n",
    "\n",
    "        img_bb_cat = (img, bboxes, categories)\n",
    "        return img_bb_cat\n",
    "\n",
    "class ImgBBoxTfm:\n",
    "    def __init__(self, output_size = 224, aug_pipline = None):\n",
    "        self.output_size = output_size\n",
    "        self.seq = aug_pipline\n",
    "    \n",
    "    def __call__(self, img_bb_cat):\n",
    "        \"\"\"\n",
    "        img_boxes: tuple of image, bboxes. bboxes are array of bbox\n",
    "        \"\"\"\n",
    "        img, bboxes, categories = img_bb_cat\n",
    "#         convert_bb_fmt = lambda bb: [bb[0], bb[1], bb[0] + bb[2], bb[1] + bb[3]] #convert topleft xy, wh to min x, min y, max x,  max y\n",
    "#         reverse_bb_fmt = lambda bb_obj: [bb_obj.x1_int, bb_obj.y1_int, int(bb_obj.width), int(bb_obj.height)]\n",
    "        extract_bb = lambda bb_obj: [bb_obj.y1_int, bb_obj.x1_int, bb_obj.y2_int, bb_obj.x2_int]\n",
    "        raw_image = img\n",
    "        raw_bbs = ia.BoundingBoxesOnImage([\n",
    "            ia.BoundingBox(*bbtfm_bb2xy(bb)) for bb in bboxes\n",
    "        ], shape=raw_image.shape)\n",
    "        \n",
    "        seq_det = self.seq.to_deterministic()\n",
    "        image_aug = seq_det.augment_images([raw_image])[0]\n",
    "        \n",
    "        bbs_aug_raw = seq_det.augment_bounding_boxes([raw_bbs])[0]\n",
    "        bbs_aug=[]\n",
    "        cate_filtered = []\n",
    "        for ix, b in enumerate(bbs_aug_raw.bounding_boxes):\n",
    "            if b.is_out_of_image(image_aug):\n",
    "                bb = [0,0,0,0] #dummy for out of image bb\n",
    "                c = 0 #assign category of out of image to BG\n",
    "            else:\n",
    "                bb = extract_bb(b.cut_out_of_image(image_aug))\n",
    "                c = categories[ix]\n",
    "            bbs_aug.append(bb)\n",
    "            cate_filtered.append(c)\n",
    "        img_bb_cat = (image_aug, bbs_aug, cate_filtered)\n",
    "        return img_bb_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "de5638226fa54b4554d1a428c2120fd4e3378d60"
   },
   "outputs": [],
   "source": [
    "def get_aug_pipline(img_size, mode = 'train'):\n",
    "    if mode == 'train':\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Scale({\"height\": img_size, \"width\": img_size}),\n",
    "            iaa.Sequential([\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Sometimes(0.5,\n",
    "                    iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "                ),\n",
    "                iaa.ContrastNormalization((0.75, 1.5)),\n",
    "                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "                iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "                iaa.Affine(\n",
    "                    rotate=(-10, 10),\n",
    "                )\n",
    "            ], random_order=True) # apply augmenters in random order\n",
    "        ], random_order=False)\n",
    "    else: #ie.val\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.Scale({\"height\": img_size, \"width\": img_size}),\n",
    "        ], random_order=False)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6084d18b14b8f427d8861a54c250b63f15f83719"
   },
   "outputs": [],
   "source": [
    "def inverse_transform(img_torch):\n",
    "    \"\"\"denormalize and inverse transform img\"\"\"\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "        std=[1/0.229, 1/0.224, 1/0.255]\n",
    "    )\n",
    "    tmp = deepcopy(img_torch)\n",
    "    inv_normalize(tmp)\n",
    "    tmp = np.clip((tmp.numpy().transpose((1,2,0)) * 255), a_min=0, a_max=255).astype(np.int)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c5eb5ec3e1fc5001b20ad57932a4d29b796f7537"
   },
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(img_anno, test_size=VAL_SIZE, random_state=42)\n",
    "\n",
    "composed = {}\n",
    "composed['train'] = transforms.Compose([ImgBBoxTfm(output_size=IMG_SIZE, aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'train')), \n",
    "                                     NormalizeImg()])\n",
    "composed['val'] = transforms.Compose([ImgBBoxTfm(output_size=IMG_SIZE, aug_pipline=get_aug_pipline(img_size=IMG_SIZE, mode = 'val')), \n",
    "                                     NormalizeImg()])\n",
    "\n",
    "image_datasets = {'train': PascalDataset(train_set, transform=composed['train']),\n",
    "                 'val': PascalDataset(val_set, transform=composed['val'])}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=1, drop_last=True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91836382ea682f99d106b56f7c06f601394d97bd"
   },
   "outputs": [],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04bb4bcfcfc5a495c6ee4e2df06de935ca423b93"
   },
   "outputs": [],
   "source": [
    "#test dataset and transformation pipline\n",
    "\n",
    "ix = 11\n",
    "tmp = image_datasets['train'][ix]\n",
    "tmp_img = tmp[0]\n",
    "tmp_bb_mask = tmp[1][1].numpy() > 0\n",
    "tmp_bboxes = tmp[1][0].numpy().reshape((-1,4))[tmp_bb_mask]\n",
    "tmp_cat = ['{}: {}'.format(int(i), img_cate_map[i]) for i in list(tmp[1][1].numpy()[tmp_bb_mask])]\n",
    "\n",
    "# show_img_bbs(inverse_transform(tmp_img), tmp_bboxes, tmp_cat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "374c95af6c8ba3a01c6ca04f98b8142a4affbce6"
   },
   "outputs": [],
   "source": [
    "def show_batch_img_bbs(imgs, bboxes, catagories):\n",
    "    thusholds = 0.4\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(12, 8))\n",
    "    for ix,ax in enumerate(axes.flat):\n",
    "        tmp_img = inverse_transform(imgs[ix])\n",
    "        tmp_bb_mask = catagories[ix].numpy() > 0\n",
    "        tmp_cate = [i for i in list(catagories[ix].numpy()[tmp_bb_mask])]\n",
    "        tmp_bboxes = bboxes[ix].numpy().reshape((-1,4))[tmp_bb_mask]\n",
    "        bb_des = ['{}: {}'.format(i, img_cate_map[i]) for i in tmp_cate]\n",
    "#         bb_tfm = [BboxCenterTfmReverse(i) for i in reverse_bb(tmp_bboxes[non_bg])]\n",
    "        ax = show_img(tmp_img, ax=ax)\n",
    "        for bb, t in zip(tmp_bboxes, bb_des):\n",
    "            draw_rect(ax, bbtfm_bb2wh(bb))\n",
    "            draw_text(ax, bb[:2], t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "962740138525589ac2dcf156dbfc102a87abc76e"
   },
   "outputs": [],
   "source": [
    "#test dataloader\n",
    "inputs, (bbox, cate) = next(iter(dataloaders['train']))\n",
    "show_batch_img_bbs(inputs, bbox, cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1702595aef04ae35ddfbfbfe6432be7fda89ff32"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    def forward(self, x): \n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class StdConv(nn.Module):\n",
    "    def __init__(self, nin, nout, stride=2, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(nin, nout, 3, stride=stride, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(nout)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x): return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "        \n",
    "def flatten_conv(x,k):\n",
    "    bs,nf,gx,gy = x.size()\n",
    "    x = x.permute(0,2,3,1).contiguous()\n",
    "    return x.view(bs,-1,nf//k)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, k, nin, bias):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.oconv1 = nn.Conv2d(nin, NUM_CLASS*k, 3, padding=1)\n",
    "        self.oconv2 = nn.Conv2d(nin, 4*k, 3, padding=1)\n",
    "        self.oconv1.bias.data.zero_().add_(bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return [\n",
    "            flatten_conv(self.oconv2(x), self.k), \n",
    "            flatten_conv(self.oconv1(x), self.k)\n",
    "                ]\n",
    "\n",
    "class RnetBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = self._prep_backbone()\n",
    "        \n",
    "    def _prep_backbone(self):     \n",
    "        base_model = models.resnet34(pretrained=True)\n",
    "        removed = list(base_model.children())[:-2]\n",
    "        backbone = nn.Sequential(*removed)\n",
    "        for param in backbone.parameters():\n",
    "            param.require_grad = False\n",
    "        return backbone\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x\n",
    "\n",
    "class SSD_MultiHead(nn.Module):\n",
    "    def __init__(self, k, bias):\n",
    "        super().__init__()\n",
    "        drop=0.4\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.sconv0 = StdConv(512,256, stride=1, drop=drop)\n",
    "        self.sconv1 = StdConv(256,256, drop=drop)\n",
    "        self.sconv2 = StdConv(256,256, drop=drop)\n",
    "        self.sconv3 = StdConv(256,256, drop=drop)\n",
    "        self.out0 = OutConv(k, 256, bias)\n",
    "        self.out1 = OutConv(k, 256, bias)\n",
    "        self.out2 = OutConv(k, 256, bias)\n",
    "        self.out3 = OutConv(k, 256, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(F.relu(x))\n",
    "        x = self.sconv0(x)\n",
    "        x = self.sconv1(x)\n",
    "        o1l, o1c = self.out1(x)\n",
    "        x = self.sconv2(x)\n",
    "        o2l, o2c = self.out2(x)\n",
    "        x = self.sconv3(x)\n",
    "        o3l, o3c = self.out3(x)\n",
    "        return [\n",
    "            torch.cat([o1l,o2l,o3l], dim=1),\n",
    "            torch.cat([o1c,o2c,o3c], dim=1)\n",
    "                ]\n",
    "    \n",
    "class RNetCustom(nn.Module):\n",
    "    def __init__(self, k, bias):\n",
    "        super().__init__()\n",
    "        self.backbone = RnetBackbone()\n",
    "        self.custom_head = SSD_MultiHead(k, bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.custom_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35cad525fa2a6f428e2c58b7ff36742a78cbee52"
   },
   "outputs": [],
   "source": [
    "# summary(model_ft, (3, IMG_SIZE, IMG_SIZE), device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8fabc258eb41fa1c6224931eda4f65246363a767"
   },
   "outputs": [],
   "source": [
    "def hw2corners(ctr, hw): return torch.cat([ctr-hw/2, ctr+hw/2], dim=1) #convert center y,x, h, w to top-left y,x bottom right y,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91ad49af24c9318e6a220505f444f5896f3ebe57"
   },
   "outputs": [],
   "source": [
    "anc_grids = [4,2,1]\n",
    "# anc_grids = [1]\n",
    "anc_zooms = [0.7, 1., 1.3]\n",
    "# anc_zooms = [1.]\n",
    "anc_ratios = [(1.,1.), (1.,0.5), (0.5,1.)]\n",
    "# anc_ratios = [(0.5,1)]\n",
    "anchor_scales = [(anz*i,anz*j) for anz in anc_zooms for (i,j) in anc_ratios]\n",
    "k = len(anchor_scales)\n",
    "anc_offsets = [1/(o*2) for o in anc_grids]\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae615cffd6f63ebfe35e5d6dd77c4fcb91d97fc1"
   },
   "outputs": [],
   "source": [
    "anc_x = np.concatenate([np.repeat(np.linspace(ao, 1-ao, ag), ag)\n",
    "                        for ao,ag in zip(anc_offsets,anc_grids)])\n",
    "anc_y = np.concatenate([np.tile(np.linspace(ao, 1-ao, ag), ag)\n",
    "                        for ao,ag in zip(anc_offsets,anc_grids)])\n",
    "anc_ctrs = np.repeat(np.stack([anc_x,anc_y], axis=1), k, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "880f3f76c17ead82118ece030080bdb4410f21d4"
   },
   "outputs": [],
   "source": [
    "anc_sizes  =   np.concatenate([np.array([[o/ag,p/ag] for i in range(ag*ag) for o,p in anchor_scales])\n",
    "               for ag in anc_grids])\n",
    "grid_sizes = torch.from_numpy(np.concatenate([np.array([ 1/ag for i in range(ag*ag) for o,p in anchor_scales])\n",
    "               for ag in anc_grids])).unsqueeze(1)\n",
    "anchors = torch.from_numpy(np.concatenate([anc_ctrs, anc_sizes], axis=1)).float()\n",
    "anchor_cnr = hw2corners(anchors[:,:2], anchors[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7011d5fd8c1b0e541f002e29054ba95bd126837"
   },
   "outputs": [],
   "source": [
    "grid_sizes=grid_sizes.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fdbd314a0a0bfec323f0c75c44ecad5b00ca6a0"
   },
   "outputs": [],
   "source": [
    "anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5589bf49321c5127beac1338b71f49ac2145106"
   },
   "outputs": [],
   "source": [
    "anchor_cnr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "836d47c72c9c9ced9500b18c4d5971be7fb11ec6"
   },
   "outputs": [],
   "source": [
    "tmp_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce16c3149f755c082f5c529e0fa65861db69a5db"
   },
   "outputs": [],
   "source": [
    "#see grid\n",
    "idx = 0\n",
    "tmp_img = inputs[idx]\n",
    "tmp_bb = anchor_cnr\n",
    "tmp_cat = list(range(len(tmp_bb)))\n",
    "show_img_bbs(inverse_transform(tmp_img), tmp_bb * IMG_SIZE, tmp_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04ed5efadc159ed840584087f3f217f2c9171509"
   },
   "outputs": [],
   "source": [
    "# grid_sizes = torch.tensor(np.array([1/anc_grid]), requires_grad=False).float().unsqueeze(1)\n",
    "# grid_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f49123c31a3c8ac1ac009404046366326d5a88f"
   },
   "outputs": [],
   "source": [
    "n_clas = NUM_CLASS\n",
    "n_act = k*(4+n_clas)\n",
    "n_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb5919e138595d63ad96d39b8dac47cc39d29351"
   },
   "outputs": [],
   "source": [
    "# prep loss\n",
    "def prep_y_true(y_true_bb, y_true_cat):\n",
    "    \"\"\"\n",
    "    prep y_true for loss calc\n",
    "    y_true_bb: shape (num bb * 4)\n",
    "    y_true_cat: shape (num bb)\n",
    "    return\n",
    "    y_true_bb: (num of bb to keep, 4)\n",
    "    y_true_cat: (num of bb to keep)\n",
    "    \"\"\"\n",
    "    bb = y_true_bb.reshape(-1, 4) #reshape to -1, 4\n",
    "    bb = bb.div(IMG_SIZE) #rescale bb dim relative to img size\n",
    "    bb_keep_mask = y_true_cat > 0 #remove bb with cat of 0\n",
    "    return bb[bb_keep_mask], y_true_cat[bb_keep_mask]\n",
    "\n",
    "def prep_y_pred_bb(y_pred_bb, anchors):\n",
    "#     print('y_pred_bb {}'.format(y_pred_bb.shape))\n",
    "    y_pred_bb = torch.tanh(y_pred_bb) #squish between -1 to +1\n",
    "    y_pred_bb_tfm_center = (y_pred_bb[:, :2] / 2 * grid_sizes) + anchors[:, :2] #adj anchor center with pred\n",
    "    y_pred_bb_tfm_hw = (y_pred_bb[:, 2:] / 2 + 1) * anchors[:, 2:] #adj anchor hw with pred\n",
    "    y_pred_bb_tfm = hw2corners(y_pred_bb_tfm_center, y_pred_bb_tfm_hw) \n",
    "    return y_pred_bb_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d4aedee2e7da78dc2e05c2fdbae29f686cbc3057"
   },
   "outputs": [],
   "source": [
    "def intersect(box_a, box_b):\n",
    "    \"\"\"\n",
    "    box_a & box_b: min-y, min-x, max-y, max-x\n",
    "    \"\"\"\n",
    "    max_xy = torch.min(box_a[:, None, 2:], box_b[None, :, 2:])\n",
    "    min_xy = torch.max(box_a[:, None, :2], box_b[None, :, :2])\n",
    "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]\n",
    "\n",
    "\n",
    "def box_sz(b): return ((b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1]))\n",
    "\n",
    "\n",
    "def jaccard(box_a, box_b):\n",
    "    \"\"\"\n",
    "    :param box_a: y_true_bbox x (min-y, min-x, max-y, max-x)\n",
    "    :param box_b: anchors x (min-y, min-x, max-y, max-x)\n",
    "    :return: iou\n",
    "    \"\"\"\n",
    "    inter = intersect(box_a, box_b)\n",
    "    union = box_sz(box_a).unsqueeze(1) + box_sz(box_b).unsqueeze(0) - inter\n",
    "    return inter / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "123f21afaa6f1f933bbbdc3f87c46256ec188386"
   },
   "outputs": [],
   "source": [
    "def map_to_ground_truth(overlaps):\n",
    "    \"\"\"\n",
    "    for each piror-box/predictions assign ground true with highest iou\n",
    "    then for each ground true assign its highest iou to piror-box;\n",
    "    \n",
    "    another way saying is\n",
    "    \n",
    "    for each ground true, assign to highest iou piror box\n",
    "    then for the rest unassigned piror box, assign ground true base highest iou\n",
    "    \"\"\"\n",
    "    prior_overlap, prior_idx = overlaps.max(1)\n",
    "    gt_overlap, gt_idx = overlaps.max(0)\n",
    "    gt_overlap[prior_idx] = 1.99\n",
    "    for i, o in enumerate(prior_idx): \n",
    "        gt_idx[o] = i\n",
    "    return gt_overlap, gt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1be979197321f853fe65e75b063c0245448783ef"
   },
   "outputs": [],
   "source": [
    "def one_hot_embedding(labels, num_classes):\n",
    "    onehot = torch.eye(num_classes)[labels.data.cpu()]\n",
    "    onehot = onehot.to(DEVICE)\n",
    "    return onehot\n",
    "\n",
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_pred_cat, y_true_cat):\n",
    "        t = one_hot_embedding(y_true_cat, NUM_CLASS)\n",
    "        t = t[:, 1:] #remove background from ground true as it doesnt count in error\n",
    "        x = y_pred_cat[:, 1:] #remove background from prediction as it doesnt count in error\n",
    "        w = self.get_weight(x,t)\n",
    "#         print('x {}, t {}'.format(x.type(), t.type()))\n",
    "        loss_cate = F.binary_cross_entropy_with_logits(x, t, w, reduction='sum') / NUM_CLASS_wo_BG\n",
    "        return loss_cate\n",
    "    \n",
    "    def get_weight(self,x,t): \n",
    "        return None\n",
    "\n",
    "class FocalLoss(BCE_Loss):\n",
    "    def get_weight(self,x,t):\n",
    "        alpha,gamma = 0.25,1\n",
    "        p = x.sigmoid()\n",
    "        pt = p*t + (1-p)*(1-t)\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        return w * (1-pt).pow(gamma)\n",
    "    \n",
    "loss_f = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4bacdb4cfc40f0188fc8bcb6b9e87340dd6a170"
   },
   "outputs": [],
   "source": [
    "#calc location loss\n",
    "def calc_ssd_loss_single(y_pred_bb, y_pred_cat, y_true_bb, y_true_cat):\n",
    "#     print('y_true_bb {}, y_pred_bb {}, y_true_cat {}, y_pred_cat {}'.format(y_true_bb.shape, y_pred_bb.shape, y_true_cat.shape, y_pred_cat.shape))\n",
    "    y_true_tfm_bb, y_true_tfm_cat = prep_y_true(y_true_bb, y_true_cat)\n",
    "    overlaps = jaccard(y_true_tfm_bb.data, anchor_cnr.data)\n",
    "    y_true_overlap, y_true_overlap_idx = map_to_ground_truth(overlaps)\n",
    "    pos = y_true_overlap > 0.4 #only count those piror-box : ground-true mapping with greater than 0.4; pos acts as a mask\n",
    "    pos_idx = torch.nonzero(pos)[:, 0] #index of pos\n",
    "    y_pred_tfm_bb = prep_y_pred_bb(y_pred_bb, anchors)\n",
    "    gt_bbox = y_pred_tfm_bb[y_true_overlap_idx]\n",
    "    bb_loss = ((y_pred_tfm_bb[pos_idx] - gt_bbox[pos_idx]).abs()).mean()\n",
    "\n",
    "    #calc classification loss\n",
    "    gt_clas = y_true_cat[y_true_overlap_idx]\n",
    "    gt_clas[1 - pos] = 0 #assign masked piror box to BG\n",
    "    cat_loss = loss_f(y_pred_cat, gt_clas)\n",
    "\n",
    "    return bb_loss, cat_loss\n",
    "\n",
    "def calc_ssd_loss_batch(y_pred, y_true):\n",
    "    cum_bb_loss, cum_cat_loss = 0., 0.\n",
    "    for y_pred_bb, y_pred_cat, y_true_bb, y_true_cat in zip(*y_pred, *y_true):\n",
    "        bb_loss, cat_loss = calc_ssd_loss_single(y_pred_bb, y_pred_cat, y_true_bb, y_true_cat)\n",
    "        cum_bb_loss += bb_loss\n",
    "        cum_cat_loss+= cat_loss\n",
    "    cum_loss = cum_bb_loss + cum_cat_loss\n",
    "    return cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3dd4479a6a08bcfbbf7451867ca2dbc40bca01c3"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, targets in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                \n",
    "                y_true_bb, y_true_cat = targets\n",
    "                y_true_bb = y_true_bb.to(DEVICE)\n",
    "                y_true_cat = y_true_cat.to(DEVICE)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, [y_true_bb, y_true_cat])\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_corrects += torch.sum(preds == categories.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            epoch_acc = 0 #for bbox only training\n",
    "    \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#         print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac8f08c4ff5f8e2558aae57c028f02cb93bdd472"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_ft = RNetCustom(k, -4.)\n",
    "model_ft = model_ft.to(DEVICE)\n",
    "anchor_cnr = anchor_cnr.to(DEVICE)\n",
    "anchors = anchors.to(DEVICE)\n",
    "grid_sizes=grid_sizes.to(DEVICE)\n",
    "criterion = calc_ssd_loss_batch\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4382ec2508cad5a5e1fc69bb9d52f52745a4c109"
   },
   "outputs": [],
   "source": [
    "#test 1\n",
    "inputs, (y_true_bb, y_true_cat) = next(iter(dataloaders['train']))\n",
    "with torch.no_grad():\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    y_pred = model_ft(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e2660d51c6e3c23a9ad6f63ae79e2188f8f485c"
   },
   "outputs": [],
   "source": [
    "y_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62a8bdc17c0bdfb3e768ac6ea33f77c947f79bba"
   },
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f18d2b3438e495567b88977a9ec3378a5e9340be"
   },
   "outputs": [],
   "source": [
    "#look at some example\n",
    "inputs, (y_true_bb, y_true_cat) = next(iter(dataloaders['val']))\n",
    "show_batch_img_bbs(inputs, y_true_bb, y_true_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1eeada5ab56a5bd84f5bd2c0f552dddfbcdbdeba"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = inputs.to(DEVICE)\n",
    "    y_pred = model_ft(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69ffc050b38ed5d80edfc8998f7ad779e44ef99d"
   },
   "outputs": [],
   "source": [
    "y_pred_bb, y_pred_cat = y_pred\n",
    "y_pred_bb = y_pred_bb.cpu()\n",
    "y_pred_cat = y_pred_cat.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "889ff090d41b179b27b871e89dedd7228f56d6a6"
   },
   "outputs": [],
   "source": [
    "idx = 8\n",
    "tmp_img = inverse_transform(inputs[idx].cpu())\n",
    "tmp_pred_bb = y_pred_bb[idx]\n",
    "tmp_pred_cat = y_pred_cat[idx]\n",
    "\n",
    "cat_p, cat_idx = F.sigmoid(tmp_pred_cat).max(1)\n",
    "\n",
    "threshold = 0.2\n",
    "tmp_mask = torch.nonzero((cat_p>0.2))[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97645c9541aa79faa629cfe3dbb1d78cc95bea55"
   },
   "outputs": [],
   "source": [
    "show_img_bbs(tmp_img, anchor_cnr[tmp_mask].cpu().numpy() * IMG_SIZE, [img_cate_map[i] for i in cat_idx[tmp_mask].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f525ea33a5c00e203d2eaf6404c0916ce5db4110"
   },
   "outputs": [],
   "source": [
    "def nms(boxes, scores, overlap=0.5, top_k=100):\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    if boxes.numel() == 0: return keep\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    v, idx = scores.sort(0)  # sort in ascending order\n",
    "    idx = idx[-top_k:]  # indices of the top-k largest vals\n",
    "    xx1 = boxes.new()\n",
    "    yy1 = boxes.new()\n",
    "    xx2 = boxes.new()\n",
    "    yy2 = boxes.new()\n",
    "    w = boxes.new()\n",
    "    h = boxes.new()\n",
    "\n",
    "    count = 0\n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]  # index of current largest val\n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        if idx.size(0) == 1: break\n",
    "        idx = idx[:-1]  # remove kept element from view\n",
    "        # load bboxes of next highest vals\n",
    "        torch.index_select(x1, 0, idx, out=xx1)\n",
    "        torch.index_select(y1, 0, idx, out=yy1)\n",
    "        torch.index_select(x2, 0, idx, out=xx2)\n",
    "        torch.index_select(y2, 0, idx, out=yy2)\n",
    "        # store element-wise max with next highest score\n",
    "        xx1 = torch.clamp(xx1, min=x1[i])\n",
    "        yy1 = torch.clamp(yy1, min=y1[i])\n",
    "        xx2 = torch.clamp(xx2, max=x2[i])\n",
    "        yy2 = torch.clamp(yy2, max=y2[i])\n",
    "        w.resize_as_(xx2)\n",
    "        h.resize_as_(yy2)\n",
    "        w = xx2 - xx1\n",
    "        h = yy2 - yy1\n",
    "        # check sizes of xx1 and xx2.. after each iteration\n",
    "        w = torch.clamp(w, min=0.0)\n",
    "        h = torch.clamp(h, min=0.0)\n",
    "        inter = w*h\n",
    "        # IoU = i / (area(a) + area(b) - i)\n",
    "        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union  # store result in iou\n",
    "        # keep only elements with an IoU <= overlap\n",
    "        idx = idx[IoU.le(overlap)]\n",
    "    return keep, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "336eb0d8c85faf8548f15d85bf6f9e360b76ca0e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
